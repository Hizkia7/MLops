name: Run MLOps Pipeline on Data Change

on:
  push:
    paths:
      - 'data/raw/**'
      - 'scripts/**'
      - 'dvc.yaml'
      - 'dvc.lock'
      - 'requirements.txt'

jobs:
  pipeline:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      MLFLOW_S3_ENDPOINT_URL: ${{ secrets.MLFLOW_S3_ENDPOINT_URL }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        pip install 'dvc[s3]'

    - name: Debug environment variables (safe)
      run: |
        echo "AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}"
        echo "MLFLOW_S3_ENDPOINT_URL=${MLFLOW_S3_ENDPOINT_URL}"

    - name: Write secrets to .env file (optional)
      run: |
        echo "AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}" >> .env
        echo "AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}" >> .env
        echo "MLFLOW_S3_ENDPOINT_URL=${MLFLOW_S3_ENDPOINT_URL}" >> .env

    - name: Set up DVC remote and pull
      run: |
        dvc remote modify myremote endpointurl $MLFLOW_S3_ENDPOINT_URL
        dvc pull

    - name: Run pipeline
      run: |
        dvc repro

    - name: Push results
      run: |
        dvc push