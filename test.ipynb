{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55c0ce6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-15 10:34:06,221 - data-acquisition - INFO - Starting data acquisition process\n",
      "2025-05-15 10:34:06,221 - data-acquisition - INFO - Creating directory data\\raw\n",
      "2025-05-15 10:34:06,222 - data-acquisition - INFO - Data file already exists at data\\raw\\creditcard-data.csv, skipping download.\n",
      "2025-05-15 10:34:06,222 - data-acquisition - WARNING - No expected checksum provided. Skipping validation.\n",
      "2025-05-15 10:34:06,224 - data-acquisition - INFO - Adding data\\raw\\creditcard-data.csv to DVC tracking...\n",
      "2025-05-15 10:34:07,567 - data-acquisition - INFO - Committing DVC changes to Git...\n",
      "Nothing to commit â€” working tree clean.\n",
      "2025-05-15 10:34:07,661 - data-acquisition - INFO - Pushing data to DVC remote...\n",
      "2025-05-15 10:34:08,998 - data-acquisition - INFO - Data acquisition completed successfully\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Data acquisition script for Credit Card Fraud Detection MLOps Pipeline.\n",
    "\n",
    "This script:\n",
    "1. Downloads the Credit Card Fraud Detection dataset\n",
    "2. Initializes DVC\n",
    "3. Adds the raw data to DVC tracking\n",
    "4. Pushes to the DVC remote\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import requests\n",
    "import hashlib\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger('data-acquisition')\n",
    "\n",
    "# Constants\n",
    "DATA_URL = \"https://nextcloud.scopicsoftware.com/s/bo5PTKgpngWymGE/download/creditcard-data.csv\"\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "RAW_DATA_FILE = RAW_DATA_DIR / \"creditcard-data.csv\"\n",
    "GIT_IGNORE = \"data/raw/.gitignore\"\n",
    "# Expected SHA256 checksum of the file (optional for validation)\n",
    "EXPECTED_SHA256 = None  # Replace with actual SHA256 if known\n",
    "\n",
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories if they don't exist.\"\"\"\n",
    "    logger.info(f\"Creating directory {RAW_DATA_DIR}\")\n",
    "    RAW_DATA_DIR.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "def download_data():\n",
    "    \"\"\"Download the dataset from the source URL.\"\"\"\n",
    "    if RAW_DATA_FILE.exists():\n",
    "        logger.info(f\"Data file already exists at {RAW_DATA_FILE}, skipping download.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Downloading data from {DATA_URL}\")\n",
    "    response = requests.get(DATA_URL, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(RAW_DATA_FILE, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    logger.info(f\"Download complete: {RAW_DATA_FILE}\")\n",
    "\n",
    "def compute_sha256(filepath):\n",
    "    \"\"\"Compute SHA256 checksum of a file.\"\"\"\n",
    "    sha256 = hashlib.sha256()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b''):\n",
    "            sha256.update(chunk)\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "def validate_data():\n",
    "    \"\"\"Validate the downloaded data file integrity.\"\"\"\n",
    "    if not RAW_DATA_FILE.exists():\n",
    "        logger.error(\"Data file does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if EXPECTED_SHA256:\n",
    "        logger.info(\"Validating data file checksum...\")\n",
    "        checksum = compute_sha256(RAW_DATA_FILE)\n",
    "        if checksum != EXPECTED_SHA256:\n",
    "            logger.error(\"Checksum does not match. File may be corrupted.\")\n",
    "            sys.exit(1)\n",
    "        logger.info(\"Checksum validated.\")\n",
    "    else:\n",
    "        logger.warning(\"No expected checksum provided. Skipping validation.\")\n",
    "\n",
    "def has_staged_changes():\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"diff\", \"--cached\", \"--quiet\"]\n",
    "    )\n",
    "    return result.returncode != 0  # True if there are staged changes\n",
    "\n",
    "def initialize_dvc():\n",
    "    \"\"\"Initialize DVC and add data to tracking.\"\"\"\n",
    "    if not Path(\".dvc\").exists():\n",
    "        logger.info(\"Initializing DVC...\")\n",
    "        subprocess.run([\"dvc\", \"init\"], check=True)\n",
    "\n",
    "    logger.info(f\"Adding {RAW_DATA_FILE} to DVC tracking...\")\n",
    "    subprocess.run([\"dvc\", \"add\", str(RAW_DATA_FILE)], check=True)\n",
    "\n",
    "    logger.info(\"Committing DVC changes to Git...\")\n",
    "    subprocess.run([\"git\", \"add\", str(RAW_DATA_FILE) + \".dvc\"], check=True)\n",
    "    subprocess.run([\"git\", \"add\", GIT_IGNORE], check=True)\n",
    "    \n",
    "    if has_staged_changes():\n",
    "        subprocess.run([\"git\", \"commit\", \"-m\", \"Add raw dataset to DVC\"], check=True)\n",
    "    else:\n",
    "        print(\"Nothing to commit â€” working tree clean.\")\n",
    "\n",
    "    logger.info(\"Pushing data to DVC remote...\")\n",
    "    subprocess.run([\"dvc\", \"push\"], check=True)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the data acquisition process.\"\"\"\n",
    "    logger.info(\"Starting data acquisition process\")\n",
    "\n",
    "    setup_directories()\n",
    "    download_data()\n",
    "    validate_data()\n",
    "    initialize_dvc()\n",
    "\n",
    "    logger.info(\"Data acquisition completed successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad17dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-15 10:47:45,272 - data-preprocessing - INFO - Check parser\n",
      "2025-05-15 10:47:45,273 - data-preprocessing - INFO - Check parser 1\n",
      "2025-05-15 10:47:45,273 - data-preprocessing - INFO - Check parser 2\n",
      "2025-05-15 10:47:45,273 - data-preprocessing - INFO - Starting data preprocessing pipeline with data revision: HEAD\n",
      "2025-05-15 10:47:45,274 - data-preprocessing - INFO - Creating processed data directory: data\\processed\n",
      "2025-05-15 10:47:45,338 - data-preprocessing - INFO - Checking out raw data at DVC revision: HEAD\n",
      "2025-05-15 10:47:47,047 - data-preprocessing - INFO - Loading dataset from data\\raw\\creditcard-data.csv\n",
      "2025-05-15 10:47:48,377 - data-preprocessing - INFO - Data summary: {'num_rows': 284807, 'num_features': 31, 'num_fraud': 492, 'num_normal': 284315}\n",
      "2025-05-15 10:47:48,381 - data-preprocessing - INFO - Splitting features and labels...\n",
      "2025-05-15 10:47:48,393 - data-preprocessing - INFO - Splitting into train/validation/test...\n",
      "2025-05-15 10:47:48,505 - data-preprocessing - INFO - Normalizing features...\n",
      "2025-05-15 10:47:48,578 - data-preprocessing - INFO - Applying SMOTE to balance the dataset...\n",
      "2025-05-15 10:47:48,610 - data-preprocessing - INFO - Saving processed datasets...\n",
      "2025-05-15 10:47:51,183 - data-preprocessing - INFO - Tracking processed data with DVC...\n",
      "ðŸƒ View run gentle-shrike-834 at: http://localhost:5000/#/experiments/1/runs/2a843446dd0d4a99a0dd54d0dabf658f\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['git', 'commit', '-m', 'Add processed datasets']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 152\u001b[39m\n\u001b[32m    149\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mData preprocessing completed successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    145\u001b[39m     stats = analyze_data(df)\n\u001b[32m    146\u001b[39m     train_df, val_df, test_df = preprocess_data(df)\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[43msave_processed_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     log_to_mlflow(stats, train_df, val_df, test_df)\n\u001b[32m    149\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mData preprocessing completed successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36msave_processed_data\u001b[39m\u001b[34m(train_df, val_df, test_df)\u001b[39m\n\u001b[32m    123\u001b[39m subprocess.run([\u001b[33m\"\u001b[39m\u001b[33mdvc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcommit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpreprocess\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m--force\u001b[39m\u001b[33m\"\u001b[39m], check=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    124\u001b[39m subprocess.run([\u001b[33m\"\u001b[39m\u001b[33mgit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33madd\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(PROCESSED_DATA_DIR)], check=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcommit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAdd processed datasets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m subprocess.run([\u001b[33m\"\u001b[39m\u001b[33mdvc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpush\u001b[39m\u001b[33m\"\u001b[39m], check=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hizkia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:571\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     retcode = process.poll()\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    572\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['git', 'commit', '-m', 'Add processed datasets']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Data preprocessing script for Credit Card Fraud Detection MLOps Pipeline.\n",
    "\n",
    "This script:\n",
    "1. Loads a specific version of raw data from DVC\n",
    "2. Handles class imbalance\n",
    "3. Normalizes features\n",
    "4. Splits data into train/validation/test sets\n",
    "5. Saves processed datasets back to DVC\n",
    "6. Logs preprocessing steps to MLflow\n",
    "\n",
    "Usage:\n",
    "    python preprocess.py --data-rev <DVC_REVISION>\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import mlflow\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger('data-preprocessing')\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "RAW_DATA_FILE = RAW_DATA_DIR / \"creditcard-data.csv\"\n",
    "\n",
    "def parse_args():\n",
    "    logger.info(f\"Check parser\")\n",
    "    parser = argparse.ArgumentParser(description='Data preprocessing script')\n",
    "    logger.info(f\"Check parser 1\")\n",
    "    parser.add_argument('--data-rev', type=str, required=False, default=\"HEAD\", help='(Optional) DVC revision/version of the raw data to use. Defaults to HEAD.')\n",
    "    logger.info(f\"Check parser 2\")\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "def setup_directories():\n",
    "    logger.info(f\"Creating processed data directory: {PROCESSED_DATA_DIR}\")\n",
    "    PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def setup_mlflow():\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_experiment(\"Preprocessing\")\n",
    "\n",
    "def load_data(data_rev):\n",
    "    logger.info(f\"Checking out raw data at DVC revision: {data_rev}\")\n",
    "    # subprocess.run([\"dvc\", \"checkout\", RAW_DATA_FILE.as_posix(), \"--rev\", data_rev], check=True)\n",
    "    subprocess.run([\"git\", \"checkout\", data_rev], check=True)\n",
    "    subprocess.run([\"dvc\", \"pull\", RAW_DATA_FILE.as_posix()], check=True)\n",
    "    logger.info(f\"Loading dataset from {RAW_DATA_FILE}\")\n",
    "    return pd.read_csv(RAW_DATA_FILE)\n",
    "\n",
    "def analyze_data(df):\n",
    "    stats = {\n",
    "        \"num_rows\": len(df),\n",
    "        \"num_features\": df.shape[1],\n",
    "        \"num_fraud\": df[df[\"Class\"] == 1].shape[0],\n",
    "        \"num_normal\": df[df[\"Class\"] == 0].shape[0],\n",
    "    }\n",
    "    mlflow.log_metrics(stats)\n",
    "    logger.info(f\"Data summary: {stats}\")\n",
    "    return stats\n",
    "\n",
    "def preprocess_data(df):\n",
    "    logger.info(\"Splitting features and labels...\")\n",
    "    X = df.drop(columns=[\"Class\"])\n",
    "    y = df[\"Class\"]\n",
    "\n",
    "    logger.info(\"Splitting into train/validation/test...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.22, random_state=42)\n",
    "\n",
    "    logger.info(\"Normalizing features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val =  scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    logger.info(\"Applying SMOTE to balance the dataset...\")\n",
    "    undersampler = RandomUnderSampler(sampling_strategy=0.1, random_state=42)\n",
    "    X_resampled, y_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    train_df = pd.DataFrame(X_resampled)\n",
    "    train_df[\"Class\"] = y_resampled.values\n",
    "\n",
    "    val_df = pd.DataFrame(X_val)\n",
    "    val_df[\"Class\"] = y_val.values\n",
    "\n",
    "    test_df = pd.DataFrame(X_test)\n",
    "    test_df[\"Class\"] = y_test.values\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def save_processed_data(train_df, val_df, test_df):\n",
    "    logger.info(\"Saving processed datasets...\")\n",
    "    train_path = PROCESSED_DATA_DIR / \"train.csv\"\n",
    "    val_path = PROCESSED_DATA_DIR / \"val.csv\"\n",
    "    test_path = PROCESSED_DATA_DIR / \"test.csv\"\n",
    "\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    val_df.to_csv(val_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "\n",
    "    logger.info(\"Tracking processed data with DVC...\")\n",
    "    # subprocess.run([\"dvc\", \"commit\", train_path.as_posix()], check=True)\n",
    "    # subprocess.run([\"dvc\", \"commit\", str(val_path)], check=True)\n",
    "    # subprocess.run([\"dvc\", \"commit\", str(test_path)], check=True)\n",
    "    subprocess.run([\"dvc\", \"commit\", \"preprocess\", \"--force\"], check=True)\n",
    "    subprocess.run([\"git\", \"add\", \".\"], check=True)\n",
    "    subprocess.run([\"git\", \"commit\", \"-m\", \"Add processed datasets\"], check=True)\n",
    "    subprocess.run([\"dvc\", \"push\"], check=True)\n",
    "\n",
    "def log_to_mlflow(stats, train_df, val_df, test_df):\n",
    "    mlflow.log_param(\"train_size\", len(train_df))\n",
    "    mlflow.log_param(\"val_size\", len(val_df))\n",
    "    mlflow.log_param(\"test_size\", len(test_df))\n",
    "    mlflow.log_metrics({\n",
    "        \"class_ratio_train\": train_df[\"Class\"].mean(),\n",
    "        \"class_ratio_val\": val_df[\"Class\"].mean(),\n",
    "        \"class_ratio_test\": test_df[\"Class\"].mean()\n",
    "    })\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    logger.info(f\"Starting data preprocessing pipeline with data revision: {args.data_rev}\")\n",
    "    setup_directories()\n",
    "    setup_mlflow()\n",
    "    with mlflow.start_run():\n",
    "        df = load_data(args.data_rev)\n",
    "        stats = analyze_data(df)\n",
    "        train_df, val_df, test_df = preprocess_data(df)\n",
    "        save_processed_data(train_df, val_df, test_df)\n",
    "        log_to_mlflow(stats, train_df, val_df, test_df)\n",
    "    logger.info(\"Data preprocessing completed successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bdffdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 17:25:54,562 - model-training - INFO - Starting model training pipeline with data revision: HEAD\n",
      "2025-05-14 17:25:54,609 - model-training - INFO - Pulling data from DVC revision: HEAD\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['dvc', 'pull', '--force', 'data/processed/train.csv']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 143\u001b[39m\n\u001b[32m    139\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mModel training completed successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    131\u001b[39m setup_directories()\n\u001b[32m    132\u001b[39m setup_mlflow()\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m X_train, y_train, X_val, y_val = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_rev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m model, best_params = train_model(X_train, y_train, X_val, y_val)\n\u001b[32m    135\u001b[39m metrics = evaluate_model(model, X_val, y_val)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(data_rev)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_data\u001b[39m(data_rev):\n\u001b[32m     67\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPulling data from DVC revision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_rev\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdvc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpull\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m--force\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROCESSED_DATA_FILE_TRAIN\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     subprocess.run([\u001b[33m\"\u001b[39m\u001b[33mdvc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m--force\u001b[39m\u001b[33m\"\u001b[39m, PROCESSED_DATA_FILE_VAL.as_posix()], check=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     70\u001b[39m     train_df = pd.read_csv(PROCESSED_DATA_DIR / \u001b[33m\"\u001b[39m\u001b[33mtrain.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hizkia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:571\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     retcode = process.poll()\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    572\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['dvc', 'pull', '--force', 'data/processed/train.csv']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Model training script for Credit Card Fraud Detection MLOps Pipeline.\n",
    "\n",
    "This script:\n",
    "1. Loads preprocessed data from a specific DVC version\n",
    "2. Trains a Gradient Boosting model (XGBoost)\n",
    "3. Performs hyperparameter tuning\n",
    "4. Tracks experiments with MLflow\n",
    "5. Registers the best model\n",
    "\n",
    "Usage:\n",
    "    python train.py --data-rev <DVC_REVISION>\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix\n",
    ")\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import subprocess\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger('model-training')\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = Path(\"data\")\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "PROCESSED_DATA_FILE_TRAIN = PROCESSED_DATA_DIR / \"train.csv\"\n",
    "PROCESSED_DATA_FILE_VAL = PROCESSED_DATA_DIR / \"val.csv\"\n",
    "MODELS_DIR = Path(\"models\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Model training script')\n",
    "    parser.add_argument('--data-rev', type=str, required=False, default=\"HEAD\",\n",
    "                        help='(Optional) DVC revision/version of the processed data to use')\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "\n",
    "def setup_directories():\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def setup_mlflow():\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_experiment(\"credit-card-fraud-detection\")\n",
    "\n",
    "\n",
    "def load_data(data_rev):\n",
    "    logger.info(f\"Pulling data from DVC revision: {data_rev}\")\n",
    "    subprocess.run([\"dvc\", \"pull\", \"--force\", PROCESSED_DATA_FILE_TRAIN.as_posix()], check=True)\n",
    "    subprocess.run([\"dvc\", \"pull\", \"--force\", PROCESSED_DATA_FILE_VAL.as_posix()], check=True)\n",
    "    train_df = pd.read_csv(PROCESSED_DATA_DIR / \"train.csv\")\n",
    "    val_df = pd.read_csv(PROCESSED_DATA_DIR / \"val.csv\")\n",
    "    X_train = train_df.drop(columns=[\"Class\"])\n",
    "    y_train = train_df[\"Class\"]\n",
    "    X_val = val_df.drop(columns=[\"Class\"])\n",
    "    y_val = val_df[\"Class\"]\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "    param_dist = {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_depth\": [3, 5, 7],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"subsample\": [0.6, 0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    logger.info(\"Starting hyperparameter tuning with RandomizedSearchCV...\")\n",
    "    search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10,\n",
    "                                scoring='roc_auc', cv=3, verbose=1, n_jobs=-1, random_state=42)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_model = search.best_estimator_\n",
    "    logger.info(f\"Best hyperparameters: {search.best_params_}\")\n",
    "    return best_model, search.best_params_\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    logger.info(\"Evaluating model on validation data...\")\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_val, y_pred),\n",
    "        \"precision\": precision_score(y_val, y_pred),\n",
    "        \"recall\": recall_score(y_val, y_pred),\n",
    "        \"f1\": f1_score(y_val, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_val, y_proba),\n",
    "        \"avg_precision\": average_precision_score(y_val, y_proba)\n",
    "    }\n",
    "    logger.info(f\"Evaluation metrics: {metrics}\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def log_to_mlflow(model, params, metrics):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.xgboost.log_model(model, \"model\")\n",
    "        mlflow.register_model(f\"runs:/{mlflow.active_run().info.run_id}/model\", \"fraud-detection-model\")\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    logger.info(\"Saving model to disk...\")\n",
    "    joblib.dump(model, MODELS_DIR / \"model.joblib\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    logger.info(f\"Starting model training pipeline with data revision: {args.data_rev}\")\n",
    "\n",
    "    setup_directories()\n",
    "    setup_mlflow()\n",
    "    X_train, y_train, X_val, y_val = load_data(args.data_rev)\n",
    "    model, best_params = train_model(X_train, y_train, X_val, y_val)\n",
    "    metrics = evaluate_model(model, X_val, y_val)\n",
    "    log_to_mlflow(model, best_params, metrics)\n",
    "    save_model(model)\n",
    "\n",
    "    logger.info(\"Model training completed successfully\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e1fdc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
