{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c0ce6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-16 14:21:35,910 - data-acquisition - INFO - Starting data acquisition process\n",
      "2025-05-16 14:21:35,910 - data-acquisition - INFO - Creating directory data\\raw\n",
      "2025-05-16 14:21:35,913 - data-acquisition - INFO - Data file already exists at data\\raw\\creditcard-data.csv, skipping download.\n",
      "2025-05-16 14:21:35,915 - data-acquisition - WARNING - No expected checksum provided. Skipping validation.\n",
      "2025-05-16 14:21:35,915 - data-acquisition - INFO - Adding data\\raw\\creditcard-data.csv to DVC tracking...\n",
      "2025-05-16 14:21:39,623 - data-acquisition - INFO - Committing DVC changes to Git...\n",
      "Nothing to commit ‚Äî working tree clean.\n",
      "2025-05-16 14:21:39,831 - data-acquisition - INFO - Pushing data to DVC remote...\n",
      "2025-05-16 14:21:42,896 - data-acquisition - INFO - Data acquisition completed successfully\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Data acquisition script for Credit Card Fraud Detection MLOps Pipeline.\n",
    "\n",
    "This script:\n",
    "1. Downloads the Credit Card Fraud Detection dataset\n",
    "2. Initializes DVC\n",
    "3. Adds the raw data to DVC tracking\n",
    "4. Pushes to the DVC remote\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import requests\n",
    "import hashlib\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger('data-acquisition')\n",
    "\n",
    "# Constants\n",
    "DATA_URL = \"https://nextcloud.scopicsoftware.com/s/bo5PTKgpngWymGE/download/creditcard-data.csv\"\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "RAW_DATA_FILE = RAW_DATA_DIR / \"creditcard-data.csv\"\n",
    "GIT_IGNORE = \"data/raw/.gitignore\"\n",
    "# Expected SHA256 checksum of the file (optional for validation)\n",
    "EXPECTED_SHA256 = None  # Replace with actual SHA256 if known\n",
    "\n",
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories if they don't exist.\"\"\"\n",
    "    logger.info(f\"Creating directory {RAW_DATA_DIR}\")\n",
    "    RAW_DATA_DIR.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "def download_data():\n",
    "    \"\"\"Download the dataset from the source URL.\"\"\"\n",
    "    if RAW_DATA_FILE.exists():\n",
    "        logger.info(f\"Data file already exists at {RAW_DATA_FILE}, skipping download.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Downloading data from {DATA_URL}\")\n",
    "    response = requests.get(DATA_URL, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(RAW_DATA_FILE, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    logger.info(f\"Download complete: {RAW_DATA_FILE}\")\n",
    "\n",
    "def compute_sha256(filepath):\n",
    "    \"\"\"Compute SHA256 checksum of a file.\"\"\"\n",
    "    sha256 = hashlib.sha256()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b''):\n",
    "            sha256.update(chunk)\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "def validate_data():\n",
    "    \"\"\"Validate the downloaded data file integrity.\"\"\"\n",
    "    if not RAW_DATA_FILE.exists():\n",
    "        logger.error(\"Data file does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if EXPECTED_SHA256:\n",
    "        logger.info(\"Validating data file checksum...\")\n",
    "        checksum = compute_sha256(RAW_DATA_FILE)\n",
    "        if checksum != EXPECTED_SHA256:\n",
    "            logger.error(\"Checksum does not match. File may be corrupted.\")\n",
    "            sys.exit(1)\n",
    "        logger.info(\"Checksum validated.\")\n",
    "    else:\n",
    "        logger.warning(\"No expected checksum provided. Skipping validation.\")\n",
    "\n",
    "def has_staged_changes():\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"diff\", \"--cached\", \"--quiet\"]\n",
    "    )\n",
    "    return result.returncode != 0  # True if there are staged changes\n",
    "\n",
    "def initialize_dvc():\n",
    "    \"\"\"Initialize DVC and add data to tracking.\"\"\"\n",
    "    if not Path(\".dvc\").exists():\n",
    "        logger.info(\"Initializing DVC...\")\n",
    "        subprocess.run([\"dvc\", \"init\"], check=True)\n",
    "\n",
    "    logger.info(f\"Adding {RAW_DATA_FILE} to DVC tracking...\")\n",
    "    subprocess.run([\"dvc\", \"add\", str(RAW_DATA_FILE)], check=True)\n",
    "\n",
    "    logger.info(\"Committing DVC changes to Git...\")\n",
    "    subprocess.run([\"git\", \"add\", str(RAW_DATA_FILE) + \".dvc\"], check=True)\n",
    "    subprocess.run([\"git\", \"add\", GIT_IGNORE], check=True)\n",
    "    \n",
    "    if has_staged_changes():\n",
    "        subprocess.run([\"git\", \"commit\", \"-m\", \"Add raw dataset to DVC\"], check=True)\n",
    "    else:\n",
    "        print(\"Nothing to commit ‚Äî working tree clean.\")\n",
    "\n",
    "    logger.info(\"Pushing data to DVC remote...\")\n",
    "    subprocess.run([\"dvc\", \"push\"], check=True)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the data acquisition process.\"\"\"\n",
    "    logger.info(\"Starting data acquisition process\")\n",
    "\n",
    "    setup_directories()\n",
    "    download_data()\n",
    "    validate_data()\n",
    "    initialize_dvc()\n",
    "\n",
    "    logger.info(\"Data acquisition completed successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad17dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:08:38,437 - data-preprocessing - INFO - Check parser\n",
      "2025-05-16 12:08:38,437 - data-preprocessing - INFO - Check parser 1\n",
      "2025-05-16 12:08:38,437 - data-preprocessing - INFO - Check parser 2\n",
      "2025-05-16 12:08:38,437 - data-preprocessing - INFO - Starting data preprocessing pipeline with data revision: HEAD\n",
      "2025-05-16 12:08:38,437 - data-preprocessing - INFO - Creating processed data directory: data\\processed\n",
      "2025-05-16 12:08:38,661 - data-preprocessing - INFO - Checking out raw data at DVC revision: HEAD\n",
      "2025-05-16 12:08:40,353 - data-preprocessing - INFO - Loading dataset from data\\raw\\creditcard-data.csv\n",
      "2025-05-16 12:08:41,751 - data-preprocessing - INFO - Data summary: {'num_rows': 284807, 'num_features': 31, 'num_fraud': 492, 'num_normal': 284315}\n",
      "2025-05-16 12:08:41,752 - data-preprocessing - INFO - Splitting features and labels...\n",
      "2025-05-16 12:08:41,765 - data-preprocessing - INFO - Splitting into train/validation/test...\n",
      "2025-05-16 12:08:41,882 - data-preprocessing - INFO - Normalizing features...\n",
      "2025-05-16 12:08:41,970 - data-preprocessing - INFO - Applying SMOTE to balance the dataset...\n",
      "2025-05-16 12:08:41,990 - data-preprocessing - INFO - Saving processed datasets...\n",
      "2025-05-16 12:08:44,585 - data-preprocessing - INFO - Tracking processed data with DVC...\n",
      "üèÉ View run orderly-lamb-712 at: http://localhost:5000/#/experiments/1/runs/e838733ab5304dceba12e506e70cd45d\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n",
      "2025-05-16 12:08:57,389 - data-preprocessing - INFO - Data preprocessing completed successfully\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Data preprocessing script for Credit Card Fraud Detection MLOps Pipeline.\n",
    "\n",
    "This script:\n",
    "1. Loads a specific version of raw data from DVC\n",
    "2. Splits data into train/validation/test sets\n",
    "3. Normalizes features\n",
    "4. Handles class imbalance\n",
    "5. Saves processed datasets back to DVC\n",
    "6. Logs preprocessing steps to MLflow\n",
    "\n",
    "Usage:\n",
    "    python preprocess.py --data-rev <DVC_REVISION>\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import mlflow\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger('data-preprocessing')\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "RAW_DATA_FILE = RAW_DATA_DIR / \"creditcard-data.csv\"\n",
    "\n",
    "def parse_args():\n",
    "    logger.info(f\"Check parser\")\n",
    "    parser = argparse.ArgumentParser(description='Data preprocessing script')\n",
    "    logger.info(f\"Check parser 1\")\n",
    "    parser.add_argument('--data-rev', type=str, required=False, default=\"HEAD\", help='(Optional) DVC revision/version of the raw data to use. Defaults to HEAD.')\n",
    "    logger.info(f\"Check parser 2\")\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "def setup_directories():\n",
    "    logger.info(f\"Creating processed data directory: {PROCESSED_DATA_DIR}\")\n",
    "    PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def setup_mlflow():\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_experiment(\"Preprocessing\")\n",
    "\n",
    "def load_data(data_rev):\n",
    "    logger.info(f\"Checking out raw data at DVC revision: {data_rev}\")\n",
    "    # subprocess.run([\"dvc\", \"checkout\", RAW_DATA_FILE.as_posix(), \"--rev\", data_rev], check=True)\n",
    "    subprocess.run([\"git\", \"checkout\", data_rev], check=True)\n",
    "    subprocess.run([\"dvc\", \"pull\", RAW_DATA_FILE.as_posix()], check=True)\n",
    "    logger.info(f\"Loading dataset from {RAW_DATA_FILE}\")\n",
    "    return pd.read_csv(RAW_DATA_FILE)\n",
    "\n",
    "def analyze_data(df):\n",
    "    stats = {\n",
    "        \"num_rows\": len(df),\n",
    "        \"num_features\": df.shape[1],\n",
    "        \"num_fraud\": df[df[\"Class\"] == 1].shape[0],\n",
    "        \"num_normal\": df[df[\"Class\"] == 0].shape[0],\n",
    "    }\n",
    "    mlflow.log_metrics(stats)\n",
    "    logger.info(f\"Data summary: {stats}\")\n",
    "    return stats\n",
    "\n",
    "def preprocess_data(df):\n",
    "    logger.info(\"Splitting features and labels...\")\n",
    "    X = df.drop(columns=[\"Class\"])\n",
    "    y = df[\"Class\"]\n",
    "\n",
    "    logger.info(\"Splitting into train/validation/test...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.22, random_state=42)\n",
    "\n",
    "    logger.info(\"Normalizing features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val =  scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    logger.info(\"Applying Downsampling to balance the dataset...\")\n",
    "    undersampler = RandomUnderSampler(sampling_strategy=0.1, random_state=42)\n",
    "    X_resampled, y_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    train_df = pd.DataFrame(X_resampled)\n",
    "    train_df[\"Class\"] = y_resampled.values\n",
    "\n",
    "    val_df = pd.DataFrame(X_val)\n",
    "    val_df[\"Class\"] = y_val.values\n",
    "\n",
    "    test_df = pd.DataFrame(X_test)\n",
    "    test_df[\"Class\"] = y_test.values\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def save_processed_data(train_df, val_df, test_df):\n",
    "    logger.info(\"Saving processed datasets...\")\n",
    "    train_path = PROCESSED_DATA_DIR / \"train.csv\"\n",
    "    val_path = PROCESSED_DATA_DIR / \"val.csv\"\n",
    "    test_path = PROCESSED_DATA_DIR / \"test.csv\"\n",
    "\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    val_df.to_csv(val_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "\n",
    "    logger.info(\"Tracking processed data with DVC...\")\n",
    "    # subprocess.run([\"dvc\", \"commit\", train_path.as_posix()], check=True)\n",
    "    # subprocess.run([\"dvc\", \"commit\", str(val_path)], check=True)\n",
    "    # subprocess.run([\"dvc\", \"commit\", str(test_path)], check=True)\n",
    "    subprocess.run([\"dvc\", \"commit\", \"preprocess\", \"--force\"], check=True)\n",
    "    time.sleep(10)\n",
    "    subprocess.run([\"git\", \"add\", \".\"], check=True)\n",
    "    subprocess.run([\"git\", \"commit\", \"-m\", \"Add processed datasets\"], check=True)\n",
    "    subprocess.run([\"dvc\", \"push\"], check=True)\n",
    "\n",
    "def log_to_mlflow(stats, train_df, val_df, test_df):\n",
    "    mlflow.log_param(\"train_size\", len(train_df))\n",
    "    mlflow.log_param(\"val_size\", len(val_df))\n",
    "    mlflow.log_param(\"test_size\", len(test_df))\n",
    "    mlflow.log_metrics({\n",
    "        \"class_ratio_train\": train_df[\"Class\"].mean(),\n",
    "        \"class_ratio_val\": val_df[\"Class\"].mean(),\n",
    "        \"class_ratio_test\": test_df[\"Class\"].mean()\n",
    "    })\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    logger.info(f\"Starting data preprocessing pipeline with data revision: {args.data_rev}\")\n",
    "    setup_directories()\n",
    "    setup_mlflow()\n",
    "    with mlflow.start_run():\n",
    "        df = load_data(args.data_rev)\n",
    "        stats = analyze_data(df)\n",
    "        train_df, val_df, test_df = preprocess_data(df)\n",
    "        save_processed_data(train_df, val_df, test_df)\n",
    "        log_to_mlflow(stats, train_df, val_df, test_df)\n",
    "    logger.info(\"Data preprocessing completed successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bdffdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_ACCESS_KEY_ID: minio\n",
      "AWS_SECRET_ACCESS_KEY: minio123\n",
      "MLFLOW_S3_ENDPOINT_URL: http://localhost:9000\n",
      "2025-05-16 12:09:03,462 - model-training - INFO - Starting model training pipeline with data revision: HEAD\n",
      "2025-05-16 12:09:03,483 - model-training - INFO - Pulling data from DVC revision: HEAD\n",
      "2025-05-16 12:09:07,147 - model-training - INFO - Starting hyperparameter tuning with RandomizedSearchCV...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hizkia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:09:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:09:14,391 - model-training - INFO - Best hyperparameters: {'subsample': 0.6, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.01}\n",
      "2025-05-16 12:09:14,392 - model-training - INFO - Evaluating model on validation data...\n",
      "2025-05-16 12:09:14,494 - model-training - INFO - Evaluation metrics: {'accuracy': 0.9990069513406157, 'precision': 0.7165354330708661, 'recall': 0.8198198198198198, 'f1': 0.7647058823529411, 'roc_auc': np.float64(0.9793036582361577), 'avg_precision': np.float64(0.6733270646807173)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hizkia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [12:09:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n",
      "2025/05/16 12:09:23 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:09:23,275 - botocore.credentials - INFO - Found credentials in environment variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'fraud-detection-model' already exists. Creating a new version of this model...\n",
      "2025/05/16 12:09:23 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: fraud-detection-model, version 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:09:23,851 - model-training - INFO - Model version 3 registered and transitioned to Staging.\n",
      "üèÉ View run hilarious-wolf-168 at: http://localhost:5000/#/experiments/2/runs/6aeffda7346047eaa7e412a23ff57f2c\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/2\n",
      "2025-05-16 12:09:23,891 - model-training - INFO - Saving model to disk...\n",
      "2025-05-16 12:09:23,895 - model-training - INFO - Model training completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '3' of model 'fraud-detection-model'.\n",
      "C:\\Users\\Hizkia\\AppData\\Local\\Temp\\ipykernel_24660\\3551093816.py:133: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Model training script for Credit Card Fraud Detection MLOps Pipeline.\n",
    "\n",
    "This script:\n",
    "1. Loads preprocessed data from a specific DVC version\n",
    "2. Trains a Gradient Boosting model (XGBoost)\n",
    "3. Performs hyperparameter tuning\n",
    "4. Tracks experiments with MLflow\n",
    "5. Registers the best model\n",
    "\n",
    "Usage:\n",
    "    python train.py --data-rev <DVC_REVISION>\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix\n",
    ")\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "from mlflow.tracking import MlflowClient\n",
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger('model-training')\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = Path(\"data\")\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "PROCESSED_DATA_FILE_TRAIN = PROCESSED_DATA_DIR / \"train.csv\"\n",
    "PROCESSED_DATA_FILE_VAL = PROCESSED_DATA_DIR / \"val.csv\"\n",
    "MODELS_DIR = Path(\"models\")\n",
    "load_dotenv()\n",
    "\n",
    "print(\"AWS_ACCESS_KEY_ID:\", os.getenv(\"AWS_ACCESS_KEY_ID\"))\n",
    "print(\"AWS_SECRET_ACCESS_KEY:\", os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "print(\"MLFLOW_S3_ENDPOINT_URL:\", os.getenv(\"MLFLOW_S3_ENDPOINT_URL\"))\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Model training script')\n",
    "    parser.add_argument('--data-rev', type=str, required=False, default=\"HEAD\",\n",
    "                        help='(Optional) DVC revision/version of the processed data to use')\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "\n",
    "def setup_directories():\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def setup_mlflow():\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_experiment(\"credit-card-fraud-detection\")\n",
    "\n",
    "\n",
    "def load_data(data_rev):\n",
    "    logger.info(f\"Pulling data from DVC revision: {data_rev}\")\n",
    "    subprocess.run([\"dvc\", \"pull\", \"--force\", PROCESSED_DATA_FILE_TRAIN.as_posix()], check=True)\n",
    "    subprocess.run([\"dvc\", \"pull\", \"--force\", PROCESSED_DATA_FILE_VAL.as_posix()], check=True)\n",
    "    train_df = pd.read_csv(PROCESSED_DATA_DIR / \"train.csv\")\n",
    "    val_df = pd.read_csv(PROCESSED_DATA_DIR / \"val.csv\")\n",
    "    X_train = train_df.drop(columns=[\"Class\"])\n",
    "    y_train = train_df[\"Class\"]\n",
    "    X_val = val_df.drop(columns=[\"Class\"])\n",
    "    y_val = val_df[\"Class\"]\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "    param_dist = {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_depth\": [3, 5, 7],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"subsample\": [0.6, 0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    logger.info(\"Starting hyperparameter tuning with RandomizedSearchCV...\")\n",
    "    search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10,\n",
    "                                scoring='roc_auc', cv=3, verbose=1, n_jobs=-1, random_state=42)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_model = search.best_estimator_\n",
    "    logger.info(f\"Best hyperparameters: {search.best_params_}\")\n",
    "    return best_model, search.best_params_\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    logger.info(\"Evaluating model on validation data...\")\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_val, y_pred),\n",
    "        \"precision\": precision_score(y_val, y_pred),\n",
    "        \"recall\": recall_score(y_val, y_pred),\n",
    "        \"f1\": f1_score(y_val, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_val, y_proba),\n",
    "        \"avg_precision\": average_precision_score(y_val, y_proba)\n",
    "    }\n",
    "    logger.info(f\"Evaluation metrics: {metrics}\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def log_to_mlflow(model, params, metrics):\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.xgboost.log_model(model, \"model\")\n",
    "\n",
    "        run_id = run.info.run_id\n",
    "        model_uri = f\"runs:/{run_id}/model\"\n",
    "        # Register the model to the MLflow Model Registry\n",
    "        registered_model = mlflow.register_model(model_uri, \"fraud-detection-model\")\n",
    "\n",
    "        # Transition the newly registered model version to \"Staging\"\n",
    "        client = MlflowClient()\n",
    "        client.transition_model_version_stage(\n",
    "            name=\"fraud-detection-model\",\n",
    "            version=registered_model.version,\n",
    "            stage=\"Staging\",\n",
    "            archive_existing_versions=True\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Model version {registered_model.version} registered and transitioned to Staging.\")\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    logger.info(\"Saving model to disk...\")\n",
    "    joblib.dump(model, MODELS_DIR / \"model.joblib\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    logger.info(f\"Starting model training pipeline with data revision: {args.data_rev}\")\n",
    "\n",
    "    setup_directories()\n",
    "    setup_mlflow()\n",
    "    X_train, y_train, X_val, y_val = load_data(args.data_rev)\n",
    "    model, best_params = train_model(X_train, y_train, X_val, y_val)\n",
    "    metrics = evaluate_model(model, X_val, y_val)\n",
    "    log_to_mlflow(model, best_params, metrics)\n",
    "    save_model(model)\n",
    "\n",
    "    logger.info(\"Model training completed successfully\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f69d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:10:13,205 - model-validation - INFO - Loading model version 'Staging' from MLflow registry...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hizkia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 2407.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:10:13,436 - model-validation - INFO - Pulling test data from DVC revision: HEAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:10:15,391 - model-validation - WARNING - Model failed to meet performance requirements for: ['min_precision', 'min_f1']\n",
      "2025-05-16 12:10:15,746 - model-validation - INFO - Validation metrics:\n",
      "2025-05-16 12:10:15,748 - model-validation - INFO - accuracy: 0.9992\n",
      "2025-05-16 12:10:15,748 - model-validation - INFO - precision: 0.7115\n",
      "2025-05-16 12:10:15,748 - model-validation - INFO - recall: 0.8043\n",
      "2025-05-16 12:10:15,749 - model-validation - INFO - f1_score: 0.7551\n",
      "2025-05-16 12:10:15,749 - model-validation - INFO - roc_auc: 0.9843\n",
      "2025-05-16 12:10:15,749 - model-validation - INFO - avg_precision: 0.6824\n",
      "üèÉ View run validation at: http://localhost:5000/#/experiments/2/runs/253cc7e42e894c4b985e6f9eac1d12a0\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/2\n",
      "2025-05-16 12:10:15,827 - model-validation - INFO - Model validation pipeline completed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Model evaluation and validation script for Credit Card Fraud Detection MLOps Pipeline.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from fastapi import FastAPI, HTTPException\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(\"model-validation\")\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = Path(\"data\")\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "PROCESSED_DATA_FILE_TEST = PROCESSED_DATA_DIR / \"test.csv\"\n",
    "\n",
    "MODELS_DIR = Path(\"models\")\n",
    "VALIDATION_DIR = MODELS_DIR / \"validation\"\n",
    "VALIDATION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PERFORMANCE_REQUIREMENTS = {\n",
    "    \"min_accuracy\": 0.98,\n",
    "    \"min_precision\": 0.85,\n",
    "    \"min_recall\": 0.70,\n",
    "    \"min_f1\": 0.75,\n",
    "    \"min_roc_auc\": 0.95\n",
    "}\n",
    "\n",
    "class InferenceInput(BaseModel):\n",
    "    inputs: Dict[str, float]\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Model validation script')\n",
    "    parser.add_argument('--model-version', type=str, required=False, default=\"Staging\")\n",
    "    parser.add_argument('--data-rev', type=str, required=False, default=\"HEAD\")\n",
    "    parser.add_argument('--start-api', action='store_true')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def setup_mlflow():\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_experiment(\"credit-card-fraud-detection\")\n",
    "\n",
    "def load_model(model_version: str):\n",
    "    logger.info(f\"Loading model version '{model_version}' from MLflow registry...\")\n",
    "    return mlflow.xgboost.load_model(f\"models:/fraud-detection-model/{model_version}\")\n",
    "\n",
    "def load_test_data(data_rev: str):\n",
    "    logger.info(f\"Pulling test data from DVC revision: {data_rev}\")\n",
    "    subprocess.run([\"dvc\", \"pull\", \"--force\", PROCESSED_DATA_FILE_TEST.as_posix()], check=True)\n",
    "\n",
    "    test_df = pd.read_csv(PROCESSED_DATA_DIR / \"test.csv\")\n",
    "    X_test = test_df.drop(columns=[\"Class\"])\n",
    "    y_test = test_df[\"Class\"]\n",
    "    return X_test, y_test\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1_score\": f1_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_proba),\n",
    "        \"avg_precision\": average_precision_score(y_test, y_proba),\n",
    "    }\n",
    "\n",
    "    return metrics, y_pred, y_proba\n",
    "\n",
    "def validate_performance(metrics: Dict[str, float]):\n",
    "    failed_metrics = [\n",
    "        key for key, val in PERFORMANCE_REQUIREMENTS.items()\n",
    "        if metrics.get(key.replace(\"min_\", \"\"), 0) < val\n",
    "    ]\n",
    "    if failed_metrics:\n",
    "        logger.warning(f\"Model failed to meet performance requirements for: {failed_metrics}\")\n",
    "        return False\n",
    "    logger.info(\"Model passed all performance requirements.\")\n",
    "    return True\n",
    "\n",
    "def create_visualizations(y_test, y_pred, y_proba):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.6)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    cm_path = VALIDATION_DIR / \"confusion_matrix.png\"\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "    roc_path = VALIDATION_DIR / \"roc_curve.png\"\n",
    "    plt.savefig(roc_path)\n",
    "    plt.close()\n",
    "\n",
    "    return [str(cm_path), str(roc_path)]\n",
    "\n",
    "def log_to_mlflow(metrics, artifacts, model_version, requirements_passed):\n",
    "    mlflow.log_param(\"validated_model_version\", model_version)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    for artifact in artifacts:\n",
    "        mlflow.log_artifact(artifact, artifact_path=\"validation\")\n",
    "    mlflow.set_tag(\"validation_passed\", requirements_passed)\n",
    "\n",
    "def setup_api(model):\n",
    "    app = FastAPI()\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    def root():\n",
    "        return {\"message\": \"Fraud Detection Model Inference API\"}\n",
    "\n",
    "    @app.post(\"/predict\")\n",
    "    def predict(input_data: InferenceInput):\n",
    "        try:\n",
    "            X = pd.DataFrame([input_data.inputs])\n",
    "            prediction = model.predict(X)[0]\n",
    "            probability = model.predict_proba(X)[0][1]\n",
    "            return {\"prediction\": int(prediction), \"probability\": float(probability)}\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "def main():\n",
    "    import sys\n",
    "    sys.argv = [sys.argv[0]]\n",
    "    args = parse_args()\n",
    "    setup_mlflow()\n",
    "\n",
    "    with mlflow.start_run(run_name=\"validation\"):\n",
    "        model = load_model(args.model_version)\n",
    "        X_test, y_test = load_test_data(args.data_rev)\n",
    "        metrics, y_pred, y_proba = evaluate_model(model, X_test, y_test)\n",
    "        requirements_passed = validate_performance(metrics)\n",
    "        artifact_paths = create_visualizations(y_test, y_pred, y_proba)\n",
    "        log_to_mlflow(metrics, artifact_paths, args.model_version, requirements_passed)\n",
    "\n",
    "        logger.info(\"Validation metrics:\")\n",
    "        for k, v in metrics.items():\n",
    "            logger.info(f\"{k}: {v:.4f}\")\n",
    "\n",
    "        if args.start_api:\n",
    "            logger.info(\"Starting model inference API...\")\n",
    "            setup_api(model)\n",
    "\n",
    "    logger.info(\"Model validation pipeline completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ed1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
